# -*- coding: utf-8 -*-
"""Customer_telcom_churn_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KxFRYPyhIz1EmhrT8pPVTa6VflfpxgBO
"""

# Commented out IPython magic to ensure Python compatibility.
#importing packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.ticker as ticker
# %matplotlib inline

#pd.options.plotting.backend='hvplot'#to hvplot

#importing datset
from google.colab import drive
drive.mount('/content/drive')

telco = pd.read_csv('/content/drive/MyDrive/Telco-Customer-Churn-prediction.csv')
telco.head(5)

telco.isnull().sum()

#ploting the null percentage
null=pd.DataFrame((telco.isnull().sum())*100/telco.shape[0]).reset_index()#shape[0] is no.of rows
plt.figure(figsize=(7,5))
sns.set_theme(style="darkgrid")
ax=sns.pointplot(x='index',y=0,data=null,color='green')#pointplot estimate the central tendency of variables
plt.title('Percentage of null values')
plt.xticks(rotation=90)
plt.xlabel('columns')
plt.ylabel('percentage')
plt.show()

"""Percentage of null values is zero"""

telco.columns

telco.shape

telco.dtypes

telco.describe()

"""Maximum tenure period customer have is 72 months. seniorcitizen didn't have proper distribution beacause it is actually categorical. Average monthly charges customer spend is 64 dolloars"""

telco.info()

#checking the value count of target variable
sns.countplot(y='Churn',data=telco,palette='rocket')
#telco['Churn'].value_counts().plot(kind='barh',figsize=(10,10),color='g')
plt.xlabel('Total counts')
plt.ylabel('Target variable')
plt.title('count of per target variable')

telco['Churn'].value_counts()*100/len(telco['Churn'])

"""Data is highly imbalance(No-73% & Yes-27%).So we have to do imbalanced learning to make evevn ratio to avoid bias.

#Data cleaning

converting datatype
"""

#creating copy of base data
telco_data = telco.copy()
telco_data.head(5)

telco_data['TotalCharges']=pd.to_numeric(telco_data['TotalCharges'],errors='coerce')#change datatype

telco_data['TotalCharges'].dtypes

telco_data['TotalCharges'].isnull().sum()

"""Totalcharges shows no null when object datatype because of nan. when converted to num it shows null."""

#filling null values
telco_data['TotalCharges']=telco_data['TotalCharges'].fillna(telco_data['TotalCharges'].mean())
telco_data['TotalCharges'].isnull().sum()

telco_data['tenure'].max() #maximum tenure period

"""Maximum tenure period is 72"""

#creating new feature
labels=["{0} - {1}".format(i,i+11) for i in range(1,72,12)]
telco_data['Tenure_period']=pd.cut(telco_data.tenure,range(1,79,12),right=False,labels=labels)

telco_data['Tenure_period'].value_counts()

"""2175 customers are between 1-12 tenure period."""

#deleting unwanted columns
telco_data.drop(columns=['customerID'],axis=1,inplace=True)
telco_data.head(3)

#detecting outlier
telco_data['TotalCharges'].describe()

telco_data['MonthlyCharges'].describe()

"""No outlier detected.

#EDA
"""

#Heatmap
plt.figure(figsize=(20,12))
sns.heatmap(telco_data.corr(),annot=True,cbar=True,linewidths=0.8,cmap='BuPu')

"""

*   column that closer to value 1 has strong linear relationship.
*   co - relation only measure linear relationship in data.
*   It doesn't measure non-linear relationship.
*   so we don't delete features based on co-reltion for other than linear relationships.

Distribution of features Continuous



"""

import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

continuous=telco_data.dtypes[telco_data.dtypes!='O'].index
continuous

telco_data.plot(kind='scatter',x="MonthlyCharges",y="TotalCharges")

sns.scatterplot(x="MonthlyCharges",y="TotalCharges",data=telco_data,hue="Churn")

"""

*   If monthly charge increases churning rate also increases.Most of the
    churning customers lies between 0-2000 Totalcharges.
*   If total charges increas monthly charge also increases. Co-relation occurs.
*   Co-relation occurs between monthlycharge and total charge.



"""

Total = sns.kdeplot(telco_data.TotalCharges[(telco_data['Churn']=='No')],color='g',shade=True)
sns.kdeplot(telco_data.TotalCharges[(telco_data['Churn']=='Yes')],ax=Total,color='r',shade=True)
plt.legend(["No churn","Churn"],loc="upper right")
plt.ylabel('Count')
plt.xlabel('Total charges')
plt.title('Totalcharges by churn')

"""When total charge is low churning rate is high.Here, we see if customer pay high amount of monthly charge in low tenure is leeds to lower total charges."""

sns.distplot(telco_data.SeniorCitizen, color='g',kde=False)

"""Majority of senior citizen is 0(No)"""

sns.distplot(telco_data.MonthlyCharges,color='g')

"""Not follow uniform distribution."""

sns.distplot(telco_data.TotalCharges,color='g')

"""Skew in data.Not follow uniform distribution.

#Distribution of features CATEGORICAL
"""

cat=[ x for x in telco_data.columns if telco_data[x].dtypes=='O']
cat

telco_data.columns

ax = sns.violinplot(x='Contract',y='SeniorCitizen',data=telco_data)

"""Most of the contract customer are Non-seniorcitizen."""

sns.countplot(x='SeniorCitizen',data=telco_data,hue='Contract')

"""Clearly shows mostof the non-senior citizens are monthly contractors and they have more likely to churn because they are free to go."""

plt.figure(figsize=(15,15))
plt.subplot(241)
plt.title("PhoneService availability")
telco_data.PhoneService.value_counts().plot(kind='bar',color='g')
plt.xticks(rotation=45)

plt.subplot(242)
plt.title("InternetService availability")
telco_data.InternetService.value_counts().plot(kind='bar',color='yellow')
plt.xticks(rotation=45)

plt.subplot(243)
plt.title("Contract")
telco_data.Contract.value_counts().plot(kind='bar',color='black')
plt.xticks(rotation=45)

plt.xticks(rotation=45)
plt.savefig('hospital.png')

plt.subplot(245)
plt.title('Partner')
telco_data.Partner.value_counts().plot(kind='bar',color='cyan')
plt.xticks(rotation=45)

"""

*   Most of the customers have phone service.
*   customers prefer month-to-month contract than year contract.
*   Most people prefer fibre optic internet service.
*   Male customers are higher than female customers.

"""

ax = sns.stripplot(x = 'gender',y='MonthlyCharges',hue='Churn',data=telco_data,palette="Set2",dodge=True)

"""*   Females are highly monthly charge customers.

#Feature Vs Target
"""

for i ,cat in enumerate(telco_data.drop(columns=["Churn","MonthlyCharges","SeniorCitizen","TotalCharges"])):
  plt.figure(i)
  sns.countplot(data=telco_data,x=cat,hue='Churn')

"""Distribution of churn with categorical features."""

telco_data['Churn']=telco_data['Churn'].map({'Yes':1,'No':0})

plt.figure(figsize=(20,14))
plt.subplot(241)
sns.barplot(x=telco_data['Contract'],y=telco_data['Churn'],order=telco_data.groupby('Contract')['Churn'].mean().reset_index().sort_values('Churn')['Contract'])
plt.title("contract Vs churn")


plt.subplot(242)
sns.barplot(x=telco_data['PhoneService'],y=telco_data['Churn'],order=telco_data.groupby('PhoneService')['Churn'].mean().reset_index().sort_values('Churn')['PhoneService'])
plt.title("PhoneService Vs churn")

plt.subplot(243)
sns.barplot(x=telco_data['MultipleLines'],y=telco_data['Churn'],order=telco_data.groupby('MultipleLines')['Churn'].mean().reset_index().sort_values('Churn')['MultipleLines'])
plt.title("MultipleLines Vs churn")

plt.subplot(244)
sns.barplot(x=telco_data['InternetService'],y=telco_data['Churn'],order=telco_data.groupby('InternetService')['Churn'].mean().reset_index().sort_values('Churn')['InternetService'])
plt.title("InternetService Vs churn")

plt.subplot(245)
sns.barplot(x=telco_data['OnlineSecurity'],y=telco_data['Churn'],order=telco_data.groupby('OnlineSecurity')['Churn'].mean().reset_index().sort_values('Churn')['OnlineSecurity'])
plt.title("OnlineSecurity Vs churn")

plt.subplot(246)
sns.barplot(x=telco_data['OnlineBackup'],y=telco_data['Churn'],order=telco_data.groupby('OnlineBackup')['Churn'].mean().reset_index().sort_values('Churn')['OnlineBackup'])
plt.title("OnlineBackup Vs churn")

plt.subplot(247)
sns.barplot(x=telco_data['DeviceProtection'],y=telco_data['Churn'],order=telco_data.groupby('DeviceProtection')['Churn'].mean().reset_index().sort_values('Churn')['DeviceProtection'])
plt.title("DeviceProtection Vs churn")

plt.subplot(248)
sns.barplot(x=telco_data['TechSupport'],y=telco_data['Churn'],order=telco_data.groupby('TechSupport')['Churn'].mean().reset_index().sort_values('Churn')['TechSupport'])
plt.title("TechSupport Vs churn")

"""

*   PhoneService follows linear relationship with churn.
*   MultipleLines follows slight linear relationship.
*   Contrsct not follow linear relationship, distribution of month contract   
    rapidly rise from other.
*   InternetService has no linear distribution. so we do one encoding when  
    fibre optic customers increase churning rate also increase.

"""

plt.figure(figsize=(19,15))
plt.subplot(241)
sns.barplot(x=telco_data['StreamingTV'],y=telco_data['Churn'],order=telco_data.groupby('StreamingTV')['Churn'].mean().reset_index().sort_values('Churn')['StreamingTV'])
plt.title("StreamingTV vs Churn")

plt.subplot(242)
sns.barplot(x=telco_data['StreamingMovies'],y=telco_data['Churn'],order=telco_data.groupby('StreamingMovies')['Churn'].mean().reset_index().sort_values('Churn')['StreamingMovies'])
plt.title("StreamingMovies vs churn")

plt.subplot(243)
sns.barplot(x=telco_data['PaymentMethod'],y=telco_data['Churn'],order=telco_data.groupby('PaymentMethod')['Churn'].mean().reset_index().sort_values('Churn')['PaymentMethod'])
plt.xticks(rotation=45)
plt.title("PaymentMethod Vs churn")

"""*   customers having no internet facility has low churning rate.
*   customers having Electric check as their payment method has more ratio to
    churning to other.
*   customers who don't stream movies are highly churners.

#Label encoding
"""

telco_data.MultipleLines.value_counts()

telco_data['gender']=telco_data['gender'].map({'Female':1,'Map':0})
telco_data['Partner']=telco_data['Partner'].map({'Yes':1,'No':0})
telco_data['Dependents']=telco_data['Dependents'].map({'Yes':1,'No':0})
telco_data['PhoneService']=telco_data['PhoneService'].map({'Yes':1,'No':0})
telco_data['PaperlessBilling']=telco_data['PaperlessBilling'].map({'Yes':1,'No':0})
telco_data['MultipleLines']=telco_data['MultipleLines'].map({'No phone service':0,'No':1,'Yes':2})

telco_data

"""#one hot encoding"""

telco_data['Tenure_period']=pd.to_numeric(telco_data['Tenure_period'],errors='coerce')

telco_data_new=pd.get_dummies(telco_data)
telco_data_new

ax = sns.countplot(x='SeniorCitizen',data=telco_data_new, hue='Churn')

"""Non senior citizens are more likely churn."""

plt.figure(figsize=(20,8))
telco_data_new.corr()['Churn'].sort_values(ascending=False).plot(kind='bar')

"""

*   Gender, phoneservice and multiple lines have no impact on churn factor.
*   contract for 2 years has very low co-relation to churn.
*   customer with **no internet service** seems like to be have very low churn.
*   High churn factors are **month contract, electronic payment, no tech
    support, no online security.**
*   Monthly contract customers are more likely to churn due to low time  
    contract terms.

"""

#telco_data_new.to_csv('Telecom_customer_churn.csv')

"""#ML Model

*   We know that our data highly imbalanced dataset.Majority(70%) of the data belongs to one class.Churning customers are --No(73%) & yes(23%)

*   So we have to do imbalanced learning to avaoid skew.
"""

!pip install imblearn

x = telco_data_new.drop(["Churn","tenure"],axis='columns')
y = telco_data_new['Churn']

import imblearn
from imblearn.combine import SMOTETomek

print(x.isna().sum())

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
x_imputed = imputer.fit_transform(x)

smote = SMOTETomek(sampling_strategy='all')
x_sm, y_sm = smote.fit_resample(x_imputed, y)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x_sm,y_sm,test_size=0.25,random_state=0) #splitting

x_train.shape,x_test.shape

from sklearn.preprocessing import StandardScaler #scaling
scaler =StandardScaler()
scaler.fit(x_train,y_train)
X_train=scaler.transform(x_train)
X_test=scaler.transform(x_test)

X_train.shape,X_test.shape

"""LogisticRegression"""

lr = LogisticRegression()
lr.fit(X_train,y_train) #fit model

y_pred=lr.predict(X_test)
y_pred

lr.predict_proba(X_test)

from sklearn.metrics import roc_auc_score,plot_roc_curve,roc_curve,confusion_matrix

#confusion_matrix(y_test,y_pred)
plot_roc_curve(lr,X_test,y_test)

acc = roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])
print("Test set auc: {:.2f}".format(acc))

"""AUCROC score is 0.94 Good model."""

confusion_matrix=pd.crosstab(y_test,y_pred,rownames=['Actual'],colnames=['Predicted'])
sns.heatmap(confusion_matrix,annot=True)

"""KNN-K Nearest Neighbours"""

from sklearn.model_selection import cross_val_score
for i in [1,2,3,4,5,6,7,8,9,10,20,50]:
  knn=KNeighborsClassifier(i)
  knn.fit(X_train,y_train)
  print("K value  : ", i, " Train Accuracy : ", knn.score(X_train,y_train)," score :  ", np.mean(cross_val_score(knn,X_train,y_train,cv=10)))

""" k=10 has the highest cross validation score. Training accuracy doesn't make any sense."""

knn= KNeighborsClassifier(10)
knn.fit(X_train,y_train)
#plot_roc_curve(knn,X_test,y_test)

#prediction
knn_pred = knn.predict(X_test)
knn_pred

#eveluation
#plot_roc_curve(knn,X_test,y_test)

acc = roc_auc_score(y_test, knn.predict_proba(X_test)[:,1])
print("Test set auc: {:.2f}".format(acc))

"""AUROC score is 0.88 Good Model.

Decision tree
"""

dtt = DecisionTreeClassifier()
dtt.fit(X_train, y_train) # it will ask all possible questions, compute the information gain and choose the best split

dtt_pred = dtt.predict(X_test)
dtt_pred

"""But for decision tree we have to set DEPTH parameter to reduce overfitting if we don't it will grow without limit that leads to overfit."""

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score
import numpy as np

for depth in [1,2,3,4,5,6,7,8,9,10,20]:
  dt = DecisionTreeClassifier(max_depth=depth)
  dt.fit(X_train,y_train)
  trainAccuracy = accuracy_score(y_train, dt.predict(X_train))
  dt = DecisionTreeClassifier(max_depth=depth)
  valAccuracy = cross_val_score(dt,X_train,y_train,cv=10)
  print("Depth  : ", depth, " Training Accuracy : ", trainAccuracy, " Cross val score : " ,np.mean(valAccuracy))

"""Depth parameter is 9."""

dt.fit(X_train,y_train)
dt_pred = dt.predict(X_test)
dt_pred

dt_acc = roc_auc_score(y_test,dt.predict_proba(X_test)[:,1])
print("Test set auc decision tree: {:.2f}".format(dt_acc))

"""0.82 is a good model"""

from matplotlib import pyplot
importance=dt.feature_importances_
pyplot.bar([x for x in range(len(importance))],importance)
list(zip(importance,x_test.columns))

"""Random forest"""

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(n_estimators=100,max_depth=3,max_features='sqrt')
rf.fit(X_train,y_train)

rf_predict=rf.predict(X_test)
rf_predict

rf_acc=roc_auc_score(y_test,rf.predict_proba(X_test)[:,1])
print("Test set auc random forest: {:.2f}".format(rf_acc))

"""0.91 is good auroc score.

XG Boost
"""

import xgboost as xgb
from sklearn.model_selection import cross_val_score
import numpy as np
for lr in [0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.11,0.12,0.13,0.14,0.15,0.2,0.5,0.7,1]:
  model = xgb.XGBClassifier(learning_rate=lr,n_estimators=100,verbosity=0)
  model.fit(X_train,y_train)

  print("Learning rate : ", lr, " Train score : ", model.score(X_train,y_train), " Cross-Val score : ", np.mean(cross_val_score(model, X_train, y_train, cv=10)))

"""learning rate =0.2 has the highest cross-val-score."""

xg_pred=model.predict(X_test)
xg_pred

xgb_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])
print("Test set auc xgb: {:.2f}".format(xgb_auc))

"""AUROC of XGB is 0.93

#comparing models


1.   AUROC of XGB is 0.93.
2.   AUROC of Random forest is 0.88
3.   AUROC of KNN is 0.93
4.   AUROC of Decision tree is 0.83
5.   AUROC of Logistic regression is 0.94

Logistic regression model has the highest AUROC score.

https://github.com/Abinaya2598/Customer_telcom_churn_prediction
"""